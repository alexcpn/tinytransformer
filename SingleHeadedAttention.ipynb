{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexcpn/tinytransformer/blob/main/SingleHeadedAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ICeONPsDSqdu",
        "outputId": "a5b4854d-955e-4c04-d5ba-d7b8713dbda4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install datasets\n",
        "!pip install  sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "44WBLfFpSwQ3"
      },
      "outputs": [],
      "source": [
        "# configure logging\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import sentencepiece as spm\n",
        "from datasets import load_dataset\n",
        "import math\n",
        "import logging as log\n",
        "import os\n",
        "import datetime as date\n",
        "\n",
        "outfile=f\"/logs/{date.datetime.now()}_simple_transformer.log\"\n",
        "\n",
        "log.basicConfig(level=log.INFO,\n",
        "                format='%(asctime)s - %(message)s',\n",
        "                datefmt='%d-%b-%y %H:%M:%S',\n",
        "                handlers=[\n",
        "                    log.FileHandler(outfile),\n",
        "                    log.StreamHandler()\n",
        "                ],\n",
        "                force=True, # Resets\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4hUag3vXU1e-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irb2lFkCVpbP",
        "outputId": "a59fcbef-5b7a-43ef-f575-e4f95cc54fde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06-Feb-25 12:59:43 - test log\n"
          ]
        }
      ],
      "source": [
        "#test log\n",
        "log.info(\"test log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g94FskKSiIc",
        "outputId": "04fba876-2d02-4506-e70f-37661ff68547"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "06-Feb-25 13:00:03 - Length of trainig data is  2119719\n",
            "06-Feb-25 13:00:06 - Limiting training legth to 100000\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"LearnTransformer\n",
        "## Learning Transformers by Doing\n",
        "\n",
        "Based on my Colab file at\n",
        "    https://colab.research.google.com/drive/1qvaWLJCenxxTcKjHksHGicxdbZDdsm7i\n",
        "\n",
        "Author: Alex Punnen and ChatGPT,CoPilot\n",
        "\n",
        "Lets see how simple self attention works by writing a single headed attention and then training them on our small dataset.\n",
        "\"\"\"\n",
        "\n",
        "# Use bpe to tokenise the sence\n",
        "\n",
        "\"\"\"It all starts with a Tokenizer that breaks words to a smaller set and creates a fixed set of vocabulary. Why fixed set vocabulary, because that is finally what is used for prediction. The model is trained to output the probability of the occurance of just the next token in say a 2000 set vocabulary. The highest probability item in that set gets selected as the next. Hence the need for a constant and fixes set vocabulary\n",
        "\n",
        "In the LLAMA Paper they are using SentencePeiece tokenize\n",
        "\n",
        "*We tokenize the data with the bytepair encoding (BPE) algorithm (Sennrich et al.,2015), using the implementation from SentencePiece\n",
        "Notably, we split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters.*\n",
        "\"\"\"\n",
        "\n",
        "# !pip install datasets\n",
        "# !pip install  sentencepiece\n",
        "\n",
        "\n",
        "\n",
        "# Load the small dataset for training our tiny language model\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")\n",
        "train_size =100000\n",
        "# use the dataset as text for training\n",
        "log.info(f\"Length of trainig data is  {len(ds['train']['text'])}\")\n",
        "# use half of this training data text\n",
        "trainingdata = ds['train']['text'][:train_size]\n",
        "log.info(f\"Limiting training legth to {len(trainingdata)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzdcxKmqT7qi",
        "outputId": "b4907900-fb64-4a11-b95b-5eb4be542195"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06-Feb-25 13:00:09 - Training Non contextual tokeniser\n",
            "06-Feb-25 13:00:24 - Sentence: The Cat sat on the Fence\n",
            "06-Feb-25 13:00:24 - Tokens:  ['The▁', 'C', 'at▁', 'sat▁', 'on▁', 'the▁', 'F', 'en', 'ce▁']\n",
            "06-Feb-25 13:00:24 - Token IDs: [60, 1947, 50, 1134, 56, 16, 1945, 23, 123]\n",
            "06-Feb-25 13:00:24 - Token ids already present in file\n",
            "06-Feb-25 13:00:32 - Total tokens:  24471344\n",
            "06-Feb-25 13:00:32 - Resizing input_ids...\n",
            "06-Feb-25 13:00:34 - input_ids.shape=torch.Size([1, 24471344])\n",
            "06-Feb-25 13:00:34 - New shape:= torch.Size([24471, 1000])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) Write the list to a file.\n",
        "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in trainingdata:\n",
        "        # replace newline with space to keep each original text chunk on a single line\n",
        "        #replace special characters\n",
        "        line = line.replace(\"â€\", \"\")\n",
        "        f.write(line.replace(\"\\n\", \" \") + \"\\n\")\n",
        "\n",
        "test_sentence = \"The Cat sat on the Fence\"\n",
        "# We use a small vocab_size just for demo. LLaMA uses a much larger vocabulary (32k tokens).\n",
        "vocab_size = 2000\n",
        "\n",
        "# if file is not there\n",
        "# this creates a vocab file and a model file\n",
        "log.info(\"Training Non contextual tokeniser\")\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=\"train.txt\",   # our training data\n",
        "    model_prefix='llama_like',\n",
        "    vocab_size=vocab_size,\n",
        "    model_type='bpe',\n",
        "    character_coverage=1.0,\n",
        "    max_sentence_length=2048,\n",
        "    treat_whitespace_as_suffix=True,\n",
        "    split_digits=True               # This forces splitting \"123\" -> \"1\", \"2\", \"3\"\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"llama_like.model\")\n",
        "\n",
        "tokens = sp.encode(test_sentence, out_type=str)\n",
        "token_ids = sp.encode(test_sentence, out_type=int)\n",
        "\n",
        "log.info(f\"Sentence: {test_sentence}\")\n",
        "log.info(f\"Tokens:  {tokens}\")\n",
        "log.info(f\"Token IDs: {token_ids}\")\n",
        "\n",
        "# get the vocabulary dictionary mapping\n",
        "# print(sp.id_to_piece(60))\n",
        "\n",
        "# Part 2\n",
        "\n",
        "# Step 1: Prepare the data for training the Attention layer\n",
        "\n",
        "# Now lets tokenise the entire text and generate a map of input_ids\n",
        "all_token_ids = []\n",
        "\n",
        "if not os.path.isfile(\"token_ids.txt\"):\n",
        "    log.info(\"Tokenizing text...\")\n",
        "    with open(\"train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            # Encode each line to token IDs\n",
        "            line_ids = sp.encode(line, out_type=int)\n",
        "            # Append them, maybe add a special token like <eol> if desired\n",
        "            all_token_ids.extend(line_ids)\n",
        "            # all_token_ids.append(eol_id)  # If you have a special EOL token\n",
        "    # Write token IDs to file\n",
        "    with open(\"token_ids.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for token_id in all_token_ids:\n",
        "            f.write(f\"{token_id}\\n\")\n",
        "else:\n",
        "    log.info(\"Token ids already present in file\")\n",
        "    #read token ids from file\n",
        "    all_token_ids = []\n",
        "    with open(\"token_ids.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            all_token_ids.append(int(line))\n",
        "\n",
        "log.info(f\"Total tokens:  {len(all_token_ids)}\")\n",
        "\n",
        "# Lets resize the input ids for training\n",
        "\n",
        "log.info(\"Resizing input_ids...\")\n",
        "\n",
        "# convert these to torch tensor\n",
        "input_ids = torch.tensor(all_token_ids, dtype=torch.long).unsqueeze(0)\n",
        "log.info(f\"input_ids.shape={input_ids.shape}\")\n",
        "# shape these (torch.Size([1, 380627])) chunk to batchsize of 1 and length of 50\n",
        "seq_length = 1000\n",
        "input_ids = input_ids.squeeze(0)  # Remove batch dim, now shape = (380627,)\n",
        "# How many 50-token chunks we can make\n",
        "num_chunks = input_ids.shape[0] // seq_length\n",
        "\n",
        "# Truncate to nearest multiple of 50\n",
        "input_ids = input_ids[:num_chunks * seq_length]\n",
        "# Reshape to (num_chunks, 50), each row is a sequence of 50 tokens\n",
        "input_ids = input_ids.view(num_chunks, seq_length)\n",
        "\n",
        "log.info(f\"New shape:= {input_ids.shape}\")  # Should be (num_chunks, 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kasm8mp4TIgP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"### Step: Adding in a Simple Attention Class\"\"\"\n",
        "\n",
        "\n",
        "class SingleHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        \"\"\"\n",
        "        d_model: dimension for Q, K, V\n",
        "        use_output_proj: if True, applies a final linear W_O\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "          Forward lyer of SingleHeadedAttention\n",
        "        \"\"\"\n",
        "        B, seq_len, d_model = x.shape  # B is batch size , seq_len is the length of the sequence , and d_model is the embedding size (512) # torch.Size([1, 999, 512])\n",
        "\n",
        "        Q = self.W_Q(x)\n",
        "        K = self.W_K(x)\n",
        "        V = self.W_V(x)\n",
        "\n",
        "        attention = torch.matmul(Q, K.transpose(-2, -1)) / \\\n",
        "            torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
        "        # Apply the mask to the attention scores\n",
        "        # why is this needed; basically it allows the model from attending to only tokens in the past, that is in the left side of\n",
        "        # the current token, when you mulitply by V\n",
        "        # the left side becomes the lower triangular matrix; and right side the future tokens are  the upper triangular matrix\n",
        "        # We build an upper-triangular mask (set to -inf) that zeros out attention (the next softmmax layer will set it to zero)\n",
        "        causal_mask = torch.triu(\n",
        "            torch.ones((seq_len, seq_len), device=x.device), diagonal=1\n",
        "        ).bool()\n",
        "        attention = attention.masked_fill(causal_mask, float('-inf'))\n",
        "        attention = torch.softmax(attention, dim=-1)\n",
        "        score = torch.matmul(attention, V)\n",
        "        # ----- [1] Add residual connection ----- ttodo take this out\n",
        "        out = x + score # without this the model output is not good\n",
        "        return out, attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HsIHmWDaTAc_"
      },
      "outputs": [],
      "source": [
        "# we need to add positional encoding to the input_ids\n",
        "# Positional encoding is a way to provide the model with information about the position of each token in the sequence.\n",
        "# This is important because the model has no inherent sense of order in the tokens, since it only sees them as embeddings.\n",
        "# generated by LLM\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len):\n",
        "        super().__init__()\n",
        "        # Create a long enough 'pe' matrix of shape [max_len, d_model]\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() *\n",
        "            (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        # Even indices (2i) -> sine\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Odd indices (2i+1) -> cosine\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register as a buffer so it's moved to GPU automatically if needed\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_len, d_model)\n",
        "        We add positional encoding up to seq_len from the precomputed 'pe'.\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        # pe[:seq_len] -> shape [seq_len, d_model]\n",
        "        # We unsqueeze(0) so that shape becomes [1, seq_len, d_model],\n",
        "        # allowing addition to x which is [batch_size, seq_len, d_model].\n",
        "        x = x + self.pe[:seq_len, :].unsqueeze(0)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yFFUC4QmUBn6"
      },
      "outputs": [],
      "source": [
        "# this will be same as labels\n",
        "labels = input_ids.clone()\n",
        "vocab_size = 2000\n",
        "d_model = 512  # embediding size\n",
        "d_k = 64  # attention size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00UE5XMsTcG4",
        "outputId": "245c5ebd-dbd3-4b38-e8f9-302a123bb847"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06-Feb-25 13:00:44 - vocab_size=2000 embedding_dim/d_model=512\n",
            "06-Feb-25 13:00:44 - Length of input ids =24471\n"
          ]
        }
      ],
      "source": [
        "log.info(f\"vocab_size={vocab_size} embedding_dim/d_model={d_model}\")\n",
        "\n",
        "# Intialise all the layers\n",
        "\n",
        "# add in the embdeiing part from previous layer\n",
        "token_embedding = nn.Embedding(\n",
        "    num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "pos_encoding = PositionalEncoding(d_model, max_len=seq_length)\n",
        "# add in the attention layer\n",
        "attention_mod = SingleHeadSelfAttention(d_model)\n",
        "# Add a linear layer for prediction\n",
        "prediction_layer1 = nn.Linear(d_model, vocab_size*2)\n",
        "prediction_layer2 = nn.Linear(vocab_size*2, vocab_size)\n",
        "layer_norm = nn.LayerNorm(vocab_size) # kast dimension is the vocab size\n",
        "\n",
        "# Define the loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "log.info(f\"Length of input ids ={len(input_ids)}\")\n",
        "\n",
        "# We'll combine these into a simple pipeline\n",
        "model = nn.ModuleList([token_embedding, pos_encoding,\n",
        "                      attention_mod,layer_norm,prediction_layer1,prediction_layer2])\n",
        "\n",
        "# The most important part is the Stochastic Gradient Descent part\n",
        "# Using model.parameters() in optimizer.step() ensures all layers, including token_embedding, attention_mod, and prediction_layer, are updated\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # SGD is unstable and hence we use this\n",
        "\n",
        "# with higher learning loss is Nan\n",
        "\n",
        "assert False == torch.isnan(input_ids).any()\n",
        "assert False == torch.isinf(input_ids).any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DyHHGP1TN-X",
        "outputId": "c262f2b7-fd8d-4a96-e285-492f29e06d3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0): Embedding(2000, 512)\n",
              "  (1): PositionalEncoding()\n",
              "  (2): SingleHeadSelfAttention(\n",
              "    (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
              "    (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
              "    (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
              "  )\n",
              "  (3): LayerNorm((2000,), eps=1e-05, elementwise_affine=True)\n",
              "  (4): Linear(in_features=512, out_features=4000, bias=True)\n",
              "  (5): Linear(in_features=4000, out_features=2000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Place all in GPU\n",
        "token_embedding.to('cuda')\n",
        "pos_encoding.to('cuda')\n",
        "attention_mod.to('cuda')\n",
        "prediction_layer1.to('cuda')\n",
        "prediction_layer2.to('cuda')\n",
        "model.to('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaFvC6UtTQ13",
        "outputId": "cd0ad354-ccef-42bb-e5d5-c481e8e80889"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06-Feb-25 13:00:49 - Training model...\n",
            "06-Feb-25 13:00:49 - N= 24471 seq_length= 1000\n",
            "06-Feb-25 13:00:49 - batch_input.shape=torch.Size([50, 1000])\n",
            "06-Feb-25 13:00:49 - batch_labels.shape=torch.Size([50, 1000])\n",
            "06-Feb-25 13:00:49 - Example input: One day, a little girl named Lily found a \n",
            "06-Feb-25 13:00:49 - Example labels: day, a little girl named Lily found a need\n",
            "06-Feb-25 13:00:50 - [Epoch=1 | Batch=0] loss=8.1334\n",
            "06-Feb-25 13:01:44 - [Epoch=1 | Batch=50] loss=4.4534\n",
            "06-Feb-25 13:02:43 - [Epoch=1 | Batch=100] loss=4.3051\n",
            "06-Feb-25 13:03:41 - [Epoch=1 | Batch=150] loss=4.2510\n",
            "06-Feb-25 13:04:40 - [Epoch=1 | Batch=200] loss=4.1468\n",
            "06-Feb-25 13:05:40 - [Epoch=1 | Batch=250] loss=3.9475\n",
            "06-Feb-25 13:06:39 - [Epoch=1 | Batch=300] loss=4.0323\n",
            "06-Feb-25 13:07:38 - [Epoch=1 | Batch=350] loss=3.6606\n",
            "06-Feb-25 13:08:37 - [Epoch=1 | Batch=400] loss=3.7789\n",
            "06-Feb-25 13:09:36 - [Epoch=1 | Batch=450] loss=3.8960\n",
            "06-Feb-25 13:10:22 - ---------Epoch 01 | Loss: 3.7440\n",
            "06-Feb-25 13:10:23 - [Epoch=2 | Batch=0] loss=3.8619\n",
            "06-Feb-25 13:11:22 - [Epoch=2 | Batch=50] loss=3.6485\n",
            "06-Feb-25 13:12:21 - [Epoch=2 | Batch=100] loss=3.7926\n",
            "06-Feb-25 13:13:21 - [Epoch=2 | Batch=150] loss=3.8854\n",
            "06-Feb-25 13:14:20 - [Epoch=2 | Batch=200] loss=3.8414\n",
            "06-Feb-25 13:15:19 - [Epoch=2 | Batch=250] loss=3.6847\n",
            "06-Feb-25 13:16:18 - [Epoch=2 | Batch=300] loss=3.7576\n",
            "06-Feb-25 13:17:18 - [Epoch=2 | Batch=350] loss=3.4321\n",
            "06-Feb-25 13:18:17 - [Epoch=2 | Batch=400] loss=3.6044\n",
            "06-Feb-25 13:19:16 - [Epoch=2 | Batch=450] loss=3.7237\n",
            "06-Feb-25 13:20:01 - ---------Epoch 02 | Loss: 3.5396\n",
            "06-Feb-25 13:20:02 - [Epoch=3 | Batch=0] loss=3.6797\n",
            "06-Feb-25 13:21:02 - [Epoch=3 | Batch=50] loss=3.4585\n",
            "06-Feb-25 13:22:01 - [Epoch=3 | Batch=100] loss=3.6178\n",
            "06-Feb-25 13:23:00 - [Epoch=3 | Batch=150] loss=3.7190\n",
            "06-Feb-25 13:23:59 - [Epoch=3 | Batch=200] loss=3.6812\n",
            "06-Feb-25 13:24:59 - [Epoch=3 | Batch=250] loss=3.5058\n",
            "06-Feb-25 13:25:58 - [Epoch=3 | Batch=300] loss=3.5928\n",
            "06-Feb-25 13:26:57 - [Epoch=3 | Batch=350] loss=3.2529\n",
            "06-Feb-25 13:27:56 - [Epoch=3 | Batch=400] loss=3.3884\n",
            "06-Feb-25 13:28:56 - [Epoch=3 | Batch=450] loss=3.5792\n",
            "06-Feb-25 13:29:41 - ---------Epoch 03 | Loss: 3.3882\n",
            "06-Feb-25 13:29:42 - [Epoch=4 | Batch=0] loss=3.5403\n",
            "06-Feb-25 13:30:41 - [Epoch=4 | Batch=50] loss=3.3151\n",
            "06-Feb-25 13:31:41 - [Epoch=4 | Batch=100] loss=3.4666\n",
            "06-Feb-25 13:32:40 - [Epoch=4 | Batch=150] loss=3.5834\n",
            "06-Feb-25 13:33:39 - [Epoch=4 | Batch=200] loss=3.5546\n",
            "06-Feb-25 13:34:38 - [Epoch=4 | Batch=250] loss=3.3596\n",
            "06-Feb-25 13:35:38 - [Epoch=4 | Batch=300] loss=3.4696\n",
            "06-Feb-25 13:36:37 - [Epoch=4 | Batch=350] loss=3.1412\n",
            "06-Feb-25 13:37:36 - [Epoch=4 | Batch=400] loss=3.3006\n",
            "06-Feb-25 13:38:35 - [Epoch=4 | Batch=450] loss=3.4577\n",
            "06-Feb-25 13:39:20 - ---------Epoch 04 | Loss: 3.2884\n",
            "06-Feb-25 13:39:21 - [Epoch=5 | Batch=0] loss=3.4361\n",
            "06-Feb-25 13:40:20 - [Epoch=5 | Batch=50] loss=3.1909\n",
            "06-Feb-25 13:41:20 - [Epoch=5 | Batch=100] loss=3.3635\n",
            "06-Feb-25 13:42:19 - [Epoch=5 | Batch=150] loss=3.4841\n",
            "06-Feb-25 13:43:18 - [Epoch=5 | Batch=200] loss=3.4665\n",
            "06-Feb-25 13:44:17 - [Epoch=5 | Batch=250] loss=3.2820\n",
            "06-Feb-25 13:45:17 - [Epoch=5 | Batch=300] loss=3.3810\n",
            "06-Feb-25 13:46:16 - [Epoch=5 | Batch=350] loss=3.0384\n",
            "06-Feb-25 13:47:15 - [Epoch=5 | Batch=400] loss=3.1926\n",
            "06-Feb-25 13:48:14 - [Epoch=5 | Batch=450] loss=3.3699\n",
            "06-Feb-25 13:48:59 - ---------Epoch 05 | Loss: 3.1956\n",
            "06-Feb-25 13:49:00 - [Epoch=6 | Batch=0] loss=3.3327\n",
            "06-Feb-25 13:49:59 - [Epoch=6 | Batch=50] loss=3.1070\n",
            "06-Feb-25 13:50:59 - [Epoch=6 | Batch=100] loss=3.2815\n",
            "06-Feb-25 13:51:58 - [Epoch=6 | Batch=150] loss=3.3946\n",
            "06-Feb-25 13:52:57 - [Epoch=6 | Batch=200] loss=3.3866\n",
            "06-Feb-25 13:53:56 - [Epoch=6 | Batch=250] loss=3.2050\n",
            "06-Feb-25 13:54:55 - [Epoch=6 | Batch=300] loss=3.3038\n",
            "06-Feb-25 13:55:55 - [Epoch=6 | Batch=350] loss=2.9563\n",
            "06-Feb-25 13:56:54 - [Epoch=6 | Batch=400] loss=3.1313\n",
            "06-Feb-25 13:57:53 - [Epoch=6 | Batch=450] loss=3.3108\n",
            "06-Feb-25 13:58:38 - ---------Epoch 06 | Loss: 3.1321\n",
            "06-Feb-25 13:58:39 - [Epoch=7 | Batch=0] loss=3.2779\n",
            "06-Feb-25 13:59:38 - [Epoch=7 | Batch=50] loss=3.0487\n",
            "06-Feb-25 14:00:38 - [Epoch=7 | Batch=100] loss=3.2328\n",
            "06-Feb-25 14:01:37 - [Epoch=7 | Batch=150] loss=3.3430\n",
            "06-Feb-25 14:02:36 - [Epoch=7 | Batch=200] loss=3.3417\n",
            "06-Feb-25 14:03:35 - [Epoch=7 | Batch=250] loss=3.1434\n",
            "06-Feb-25 14:04:34 - [Epoch=7 | Batch=300] loss=3.2613\n",
            "06-Feb-25 14:05:34 - [Epoch=7 | Batch=350] loss=2.9306\n",
            "06-Feb-25 14:06:33 - [Epoch=7 | Batch=400] loss=3.0876\n",
            "06-Feb-25 14:07:32 - [Epoch=7 | Batch=450] loss=3.2557\n",
            "06-Feb-25 14:08:17 - ---------Epoch 07 | Loss: 3.0790\n",
            "06-Feb-25 14:08:18 - [Epoch=8 | Batch=0] loss=3.2172\n",
            "06-Feb-25 14:09:17 - [Epoch=8 | Batch=50] loss=2.9968\n",
            "06-Feb-25 14:10:17 - [Epoch=8 | Batch=100] loss=3.1722\n",
            "06-Feb-25 14:11:16 - [Epoch=8 | Batch=150] loss=3.3054\n",
            "06-Feb-25 14:12:15 - [Epoch=8 | Batch=200] loss=3.2906\n",
            "06-Feb-25 14:13:14 - [Epoch=8 | Batch=250] loss=3.0985\n",
            "06-Feb-25 14:14:13 - [Epoch=8 | Batch=300] loss=3.2128\n",
            "06-Feb-25 14:15:13 - [Epoch=8 | Batch=350] loss=2.8758\n",
            "06-Feb-25 14:16:12 - [Epoch=8 | Batch=400] loss=3.0058\n",
            "06-Feb-25 14:17:11 - [Epoch=8 | Batch=450] loss=3.2197\n",
            "06-Feb-25 14:17:56 - ---------Epoch 08 | Loss: 3.0329\n",
            "06-Feb-25 14:17:57 - [Epoch=9 | Batch=0] loss=3.1767\n",
            "06-Feb-25 14:18:56 - [Epoch=9 | Batch=50] loss=2.9571\n",
            "06-Feb-25 14:19:56 - [Epoch=9 | Batch=100] loss=3.1320\n",
            "06-Feb-25 14:20:55 - [Epoch=9 | Batch=150] loss=3.2588\n",
            "06-Feb-25 14:21:54 - [Epoch=9 | Batch=200] loss=3.2626\n",
            "06-Feb-25 14:22:53 - [Epoch=9 | Batch=250] loss=3.0624\n",
            "06-Feb-25 14:23:52 - [Epoch=9 | Batch=300] loss=3.1723\n",
            "06-Feb-25 14:24:51 - [Epoch=9 | Batch=350] loss=2.8390\n",
            "06-Feb-25 14:25:50 - [Epoch=9 | Batch=400] loss=2.9777\n",
            "06-Feb-25 14:26:50 - [Epoch=9 | Batch=450] loss=3.1823\n",
            "06-Feb-25 14:27:35 - ---------Epoch 09 | Loss: 3.0124\n",
            "06-Feb-25 14:27:36 - [Epoch=10 | Batch=0] loss=3.1513\n",
            "06-Feb-25 14:28:35 - [Epoch=10 | Batch=50] loss=2.9287\n",
            "06-Feb-25 14:29:34 - [Epoch=10 | Batch=100] loss=3.0994\n",
            "06-Feb-25 14:30:33 - [Epoch=10 | Batch=150] loss=3.2188\n",
            "06-Feb-25 14:31:33 - [Epoch=10 | Batch=200] loss=3.2207\n",
            "06-Feb-25 14:32:32 - [Epoch=10 | Batch=250] loss=3.0425\n",
            "06-Feb-25 14:33:31 - [Epoch=10 | Batch=300] loss=3.1409\n",
            "06-Feb-25 14:34:31 - [Epoch=10 | Batch=350] loss=2.8250\n",
            "06-Feb-25 14:35:30 - [Epoch=10 | Batch=400] loss=2.9598\n",
            "06-Feb-25 14:36:29 - [Epoch=10 | Batch=450] loss=3.1620\n",
            "06-Feb-25 14:37:14 - ---------Epoch 10 | Loss: 2.9886\n"
          ]
        }
      ],
      "source": [
        "log.info(\"Training model...\")\n",
        "\n",
        "model.train()\n",
        "batch_size = 50\n",
        "N, seq_length = input_ids.shape\n",
        "log.info(f\"N= {N} seq_length= {seq_length}\")\n",
        "num_batches = N // batch_size\n",
        "\n",
        "for epoch in range(10):\n",
        "    for start_idx in range(0, N, batch_size):\n",
        "        end_idx = start_idx + batch_size\n",
        "        if end_idx > N:\n",
        "            break  # in case N not multiple of batch_size\n",
        "\n",
        "        # Slice out a batch\n",
        "        batch_input = input_ids[start_idx:end_idx, :]   # (B, seq_length)\n",
        "        batch_labels = labels[start_idx:end_idx, :]     # (B, seq_length)\n",
        "        if epoch == 0 and start_idx == 0:\n",
        "            log.info(f\"batch_input.shape={batch_input.shape}\")\n",
        "            log.info(f\"batch_labels.shape={batch_labels.shape}\")\n",
        "\n",
        "        # Move to GPU\n",
        "        batch_input = batch_input.to('cuda')\n",
        "        batch_labels = batch_labels.to('cuda')\n",
        "\n",
        "        # 1) Shift input & labels so model predicts next token\n",
        "        #    shape -> (B, seq_length-1)\n",
        "        trimmed_input = batch_input[:, :-1]\n",
        "        target_labels = batch_labels[:, 1:]\n",
        "        if epoch == 0 and start_idx == 0:\n",
        "            # take 10 tokens\n",
        "            log.info(\"Example input: %s\", sp.decode(trimmed_input[0].tolist()[:10]))\n",
        "            log.info(\"Example labels: %s\",sp.decode(target_labels[0].tolist()[:10]))\n",
        "\n",
        "        embedded_tokens = token_embedding(trimmed_input)\n",
        "        # shape remains (batch_size, seq_len, d_model)\n",
        "        pos_embedded_tokens = pos_encoding(embedded_tokens)\n",
        "        # get attention and score\n",
        "        score,_ = attention_mod(pos_embedded_tokens)\n",
        "        # Predict the next word\n",
        "        hidden1 = prediction_layer1(score)  # Project to vocabulary size\n",
        "        logits = prediction_layer2(hidden1)  # through few linear layers\n",
        "        # add layer norm\n",
        "        logits = layer_norm(logits)\n",
        "        # the last dimension of the output tensor represents the vocabulary size or the number of classes.\n",
        "        # Therefore, applying softmax along the last dimension (dim=-1)\n",
        "        predicted_probs = torch.softmax(logits, dim=-1)  # Get probabilities\n",
        "        # Get the predicted word (token ID)\n",
        "        predicted_token_id = torch.argmax(predicted_probs, dim=-1)\n",
        "        # Calculate the loss # crossentropy already does softmax inside\n",
        "        # If your input has 49 tokens, you predict 49 next tokens.\n",
        "        loss = loss_function(\n",
        "            logits.reshape(-1, vocab_size),\n",
        "            target_labels.reshape(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "        # We are not discarding the loss or ignoring it; rather, we’re enforcing a limit on the size of the update to avoid erratic jumps.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        # print training progress occasionally\n",
        "        if (start_idx // batch_size) % 50 == 0:\n",
        "            log.info(\"[Epoch=%d | Batch=%d] loss=%.4f\", epoch+1, start_idx//batch_size, loss.item())\n",
        "        if loss.item() < 0.5:\n",
        "            break\n",
        "        # free gpu memory\n",
        "        del batch_input\n",
        "        del batch_labels\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    log.info(f\"---------Epoch {epoch+1:02d} | Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd-nGb8hTCoG",
        "outputId": "a1ab8d21-dc75-4ebb-ac5c-7164e8ee5dba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06-Feb-25 14:37:34 - Model weights saved\n",
            "06-Feb-25 14:37:34 - Generated Text=Bloom lived in a big garden with lots of funny. One day, Lily saw a big tree branch of funny's friends decided to play with. They were very much. They were very happy that they had lots of funny's friends were very happy that they had lots of funny's friends were very happy that they had lots of funny't always wanted to play with. They were very happy that they had lots of funny't always wanted to play with. They were very much\n"
          ]
        }
      ],
      "source": [
        "\"\"\"# Use the trained model to predict\"\"\"\n",
        "\n",
        "# save the model weights\n",
        "torch.save(model.state_dict(), f\"./weights/{date.datetime.now()}_model_weights.pth\")\n",
        "log.info(\"Model weights saved\")\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Test the generation function\n",
        "prompt = \"Bloom lived in a big garden\"\n",
        "\n",
        "generated_tokens = sp.encode(prompt, out_type=int)  # Tokenize input text\n",
        "\n",
        "# Convert to tensor\n",
        "input_tensor = torch.tensor(\n",
        "    generated_tokens, dtype=torch.long).unsqueeze(0)  # (1, seq_length)\n",
        "max_length = 100\n",
        "for _ in range(max_length):\n",
        "    # Get embedding\n",
        "    embedded_tokens = token_embedding(input_tensor.to('cuda'))\n",
        "    # Get attention and score\n",
        "    score, attention = attention_mod(embedded_tokens)\n",
        "    # Predict the next word\n",
        "    hidden1 = prediction_layer1(score)  # (1, seq_length, vocab_size)\n",
        "    logits = prediction_layer2(hidden1)  # (1, seq_length, vocab_size)\n",
        "    logits = layer_norm(logits)\n",
        "    # Get the last token's logits (for autoregressive prediction)\n",
        "    next_token_logits = logits[:, -1, :]  # Shape: (1, vocab_size)\n",
        "    # Convert logits to token probabilities\n",
        "    next_token_id = torch.argmax(next_token_logits, dim=-1)  # (1,)\n",
        "    # Append new token\n",
        "    generated_tokens.append(next_token_id.item())\n",
        "    # Stop if we generate an EOS token (optional)\n",
        "    if next_token_id.item() == sp.eos_id():  # Ensure your tokenizer has an EOS token\n",
        "        break\n",
        "\n",
        "    # Update input tensor with new token for next iteration\n",
        "    input_tensor = torch.tensor(\n",
        "        generated_tokens, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "# Decode generated token IDs back to text\n",
        "generated_text = sp.decode(generated_tokens)\n",
        "log.info(f\"Generated Text={generated_text}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNcdjdRy96E0eP1Wd0qToa6",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
