06-Feb-25 18:05:34 - Length of trainig data is  2119719
06-Feb-25 18:05:36 - Limiting training legth to 100000
06-Feb-25 18:05:36 - Training Non contextual tokeniser
06-Feb-25 18:05:41 - Sentence: The Cat sat on the Fence
06-Feb-25 18:05:41 - Tokens:  ['The▁', 'C', 'at▁', 'sat▁', 'on▁', 'the▁', 'F', 'en', 'ce▁']
06-Feb-25 18:05:41 - Token IDs: [60, 1947, 50, 1134, 56, 16, 1945, 23, 123]
06-Feb-25 18:05:41 - Token ids already present in file
06-Feb-25 18:05:45 - Total tokens:  24471344
06-Feb-25 18:05:45 - Resizing input_ids...
06-Feb-25 18:05:46 - input_ids.shape=torch.Size([1, 24471344])
06-Feb-25 18:05:46 - New shape:= torch.Size([24471, 1000])
06-Feb-25 18:05:46 - vocab_size=2000 embedding_dim/d_model=512
06-Feb-25 18:05:46 - Length of input ids =24471
06-Feb-25 18:05:48 - Training model...
06-Feb-25 18:05:48 - N= 24471 seq_length= 1000
06-Feb-25 18:05:48 - batch_input.shape=torch.Size([50, 1000])
06-Feb-25 18:05:48 - batch_labels.shape=torch.Size([50, 1000])
06-Feb-25 18:05:48 - Example input: One day, a little girl named Lily found a 
06-Feb-25 18:05:48 - Example labels: day, a little girl named Lily found a need
06-Feb-25 18:05:48 - [Epoch=1 | Batch=0] loss=8.1075
06-Feb-25 18:06:26 - [Epoch=1 | Batch=50] loss=4.4464
06-Feb-25 18:07:04 - [Epoch=1 | Batch=100] loss=4.2713
06-Feb-25 18:07:43 - [Epoch=1 | Batch=150] loss=4.3047
06-Feb-25 18:08:22 - [Epoch=1 | Batch=200] loss=4.1749
06-Feb-25 18:09:03 - [Epoch=1 | Batch=250] loss=3.9689
06-Feb-25 18:09:44 - [Epoch=1 | Batch=300] loss=4.0351
06-Feb-25 18:10:24 - [Epoch=1 | Batch=350] loss=3.7007
06-Feb-25 18:11:04 - [Epoch=1 | Batch=400] loss=3.8601
06-Feb-25 18:11:45 - [Epoch=1 | Batch=450] loss=3.9109
06-Feb-25 18:12:17 - ---------Epoch 01 | Loss: 3.7388
06-Feb-25 18:12:18 - [Epoch=2 | Batch=0] loss=3.8701
06-Feb-25 18:12:59 - [Epoch=2 | Batch=50] loss=3.6182
06-Feb-25 18:13:39 - [Epoch=2 | Batch=100] loss=3.7965
06-Feb-25 18:14:20 - [Epoch=2 | Batch=150] loss=3.8938
06-Feb-25 18:15:01 - [Epoch=2 | Batch=200] loss=3.8518
06-Feb-25 18:15:43 - [Epoch=2 | Batch=250] loss=3.7304
06-Feb-25 18:16:24 - [Epoch=2 | Batch=300] loss=3.7850
06-Feb-25 18:17:05 - [Epoch=2 | Batch=350] loss=3.4403
06-Feb-25 18:17:47 - [Epoch=2 | Batch=400] loss=3.5871
06-Feb-25 18:18:29 - [Epoch=2 | Batch=450] loss=3.7294
06-Feb-25 18:19:01 - ---------Epoch 02 | Loss: 3.5387
06-Feb-25 18:19:02 - [Epoch=3 | Batch=0] loss=3.6817
06-Feb-25 18:19:46 - [Epoch=3 | Batch=50] loss=3.4475
06-Feb-25 18:20:28 - [Epoch=3 | Batch=100] loss=3.6457
06-Feb-25 18:21:10 - [Epoch=3 | Batch=150] loss=3.7276
06-Feb-25 18:21:52 - [Epoch=3 | Batch=200] loss=3.7111
06-Feb-25 18:22:34 - [Epoch=3 | Batch=250] loss=3.5104
06-Feb-25 18:23:15 - [Epoch=3 | Batch=300] loss=3.6110
06-Feb-25 18:23:57 - [Epoch=3 | Batch=350] loss=3.2722
06-Feb-25 18:24:38 - [Epoch=3 | Batch=400] loss=3.3933
06-Feb-25 18:25:20 - [Epoch=3 | Batch=450] loss=3.5753
06-Feb-25 18:25:53 - ---------Epoch 03 | Loss: 3.3854
06-Feb-25 18:25:54 - [Epoch=4 | Batch=0] loss=3.5326
06-Feb-25 18:26:36 - [Epoch=4 | Batch=50] loss=3.3071
06-Feb-25 18:27:18 - [Epoch=4 | Batch=100] loss=3.4799
06-Feb-25 18:28:00 - [Epoch=4 | Batch=150] loss=3.5845
06-Feb-25 18:28:42 - [Epoch=4 | Batch=200] loss=3.5614
06-Feb-25 18:29:23 - [Epoch=4 | Batch=250] loss=3.3614
06-Feb-25 18:30:05 - [Epoch=4 | Batch=300] loss=3.4818
06-Feb-25 18:30:48 - [Epoch=4 | Batch=350] loss=3.1342
06-Feb-25 18:31:33 - [Epoch=4 | Batch=400] loss=3.2886
06-Feb-25 18:32:17 - [Epoch=4 | Batch=450] loss=3.4561
06-Feb-25 18:32:51 - ---------Epoch 04 | Loss: 3.2888
06-Feb-25 18:32:52 - [Epoch=5 | Batch=0] loss=3.4284
06-Feb-25 18:33:37 - [Epoch=5 | Batch=50] loss=3.1915
06-Feb-25 18:34:21 - [Epoch=5 | Batch=100] loss=3.3716
06-Feb-25 18:35:06 - [Epoch=5 | Batch=150] loss=3.4687
06-Feb-25 18:35:49 - [Epoch=5 | Batch=200] loss=3.4661
06-Feb-25 18:36:31 - [Epoch=5 | Batch=250] loss=3.2803
06-Feb-25 18:37:13 - [Epoch=5 | Batch=300] loss=3.3849
06-Feb-25 18:37:56 - [Epoch=5 | Batch=350] loss=3.0471
06-Feb-25 18:38:39 - [Epoch=5 | Batch=400] loss=3.1786
06-Feb-25 18:39:21 - [Epoch=5 | Batch=450] loss=3.3650
06-Feb-25 18:39:54 - ---------Epoch 05 | Loss: 3.1814
06-Feb-25 18:39:55 - [Epoch=6 | Batch=0] loss=3.3228
06-Feb-25 18:40:37 - [Epoch=6 | Batch=50] loss=3.1025
06-Feb-25 18:41:18 - [Epoch=6 | Batch=100] loss=3.2736
06-Feb-25 18:41:59 - [Epoch=6 | Batch=150] loss=3.4032
06-Feb-25 18:42:39 - [Epoch=6 | Batch=200] loss=3.3809
06-Feb-25 18:43:21 - [Epoch=6 | Batch=250] loss=3.1873
06-Feb-25 18:44:02 - [Epoch=6 | Batch=300] loss=3.3011
06-Feb-25 18:44:44 - [Epoch=6 | Batch=350] loss=2.9771
06-Feb-25 18:45:25 - [Epoch=6 | Batch=400] loss=3.1402
06-Feb-25 18:46:06 - [Epoch=6 | Batch=450] loss=3.2906
06-Feb-25 18:46:37 - ---------Epoch 06 | Loss: 3.1284
06-Feb-25 18:46:38 - [Epoch=7 | Batch=0] loss=3.2594
06-Feb-25 18:47:19 - [Epoch=7 | Batch=50] loss=3.0408
06-Feb-25 18:48:00 - [Epoch=7 | Batch=100] loss=3.2273
06-Feb-25 18:48:41 - [Epoch=7 | Batch=150] loss=3.3384
06-Feb-25 18:49:23 - [Epoch=7 | Batch=200] loss=3.3322
06-Feb-25 18:50:03 - [Epoch=7 | Batch=250] loss=3.1411
06-Feb-25 18:50:44 - [Epoch=7 | Batch=300] loss=3.2511
06-Feb-25 18:51:24 - [Epoch=7 | Batch=350] loss=2.9276
06-Feb-25 18:52:05 - [Epoch=7 | Batch=400] loss=3.0804
06-Feb-25 18:52:46 - [Epoch=7 | Batch=450] loss=3.2619
06-Feb-25 18:53:16 - ---------Epoch 07 | Loss: 3.0855
06-Feb-25 18:53:17 - [Epoch=8 | Batch=0] loss=3.2198
06-Feb-25 18:53:57 - [Epoch=8 | Batch=50] loss=3.0010
06-Feb-25 18:54:37 - [Epoch=8 | Batch=100] loss=3.1828
06-Feb-25 18:55:18 - [Epoch=8 | Batch=150] loss=3.2914
06-Feb-25 18:55:58 - [Epoch=8 | Batch=200] loss=3.2910
06-Feb-25 18:56:38 - [Epoch=8 | Batch=250] loss=3.0980
06-Feb-25 18:57:18 - [Epoch=8 | Batch=300] loss=3.2238
06-Feb-25 18:57:58 - [Epoch=8 | Batch=350] loss=2.8835
06-Feb-25 18:58:38 - [Epoch=8 | Batch=400] loss=3.0599
06-Feb-25 18:59:18 - [Epoch=8 | Batch=450] loss=3.2141
06-Feb-25 18:59:48 - ---------Epoch 08 | Loss: 3.0382
06-Feb-25 18:59:49 - [Epoch=9 | Batch=0] loss=3.1746
06-Feb-25 19:00:29 - [Epoch=9 | Batch=50] loss=2.9603
06-Feb-25 19:01:09 - [Epoch=9 | Batch=100] loss=3.1375
06-Feb-25 19:01:49 - [Epoch=9 | Batch=150] loss=3.2640
06-Feb-25 19:02:29 - [Epoch=9 | Batch=200] loss=3.2478
06-Feb-25 19:03:09 - [Epoch=9 | Batch=250] loss=3.0543
06-Feb-25 19:03:49 - [Epoch=9 | Batch=300] loss=3.1844
06-Feb-25 19:04:29 - [Epoch=9 | Batch=350] loss=2.8432
06-Feb-25 19:05:09 - [Epoch=9 | Batch=400] loss=3.0071
06-Feb-25 19:05:49 - [Epoch=9 | Batch=450] loss=3.1816
06-Feb-25 19:06:19 - ---------Epoch 09 | Loss: 2.9973
06-Feb-25 19:06:20 - [Epoch=10 | Batch=0] loss=3.1356
06-Feb-25 19:07:00 - [Epoch=10 | Batch=50] loss=2.9191
06-Feb-25 19:07:40 - [Epoch=10 | Batch=100] loss=3.0974
06-Feb-25 19:08:21 - [Epoch=10 | Batch=150] loss=3.2040
06-Feb-25 19:09:01 - [Epoch=10 | Batch=200] loss=3.2259
06-Feb-25 19:09:41 - [Epoch=10 | Batch=250] loss=3.0232
06-Feb-25 19:10:21 - [Epoch=10 | Batch=300] loss=3.1497
06-Feb-25 19:11:02 - [Epoch=10 | Batch=350] loss=2.8186
06-Feb-25 19:11:42 - [Epoch=10 | Batch=400] loss=2.9705
06-Feb-25 19:12:22 - [Epoch=10 | Batch=450] loss=3.1599
06-Feb-25 19:12:52 - ---------Epoch 10 | Loss: 2.9738
06-Feb-25 19:12:52 - Model weights saved
06-Feb-25 19:12:52 - Generated Text=Bloom lived in a big garden with lots of funny. He was so excited 
06-Feb-25 20:55:36 - Length of trainig data is  2119719
06-Feb-25 20:55:38 - Limiting training legth to 100000
06-Feb-25 20:55:38 - Training Non contextual tokeniser
06-Feb-25 20:55:43 - Sentence: The Cat sat on the Fence
06-Feb-25 20:55:43 - Tokens:  ['The▁', 'C', 'at▁', 'sat▁', 'on▁', 'the▁', 'F', 'en', 'ce▁']
06-Feb-25 20:55:43 - Token IDs: [60, 1947, 50, 1134, 56, 16, 1945, 23, 123]
06-Feb-25 20:55:43 - Token ids already present in file
06-Feb-25 20:55:46 - Total tokens:  24471344
06-Feb-25 20:55:46 - Resizing input_ids...
06-Feb-25 20:55:47 - input_ids.shape=torch.Size([1, 24471344])
06-Feb-25 20:55:47 - New shape:= torch.Size([24471, 1000])
06-Feb-25 20:55:47 - vocab_size=2000 embedding_dim/d_model=512
06-Feb-25 20:55:47 - Length of input ids =24471
06-Feb-25 20:55:48 - Training model...
06-Feb-25 20:55:48 - N= 24471 seq_length= 1000
06-Feb-25 20:55:48 - batch_input.shape=torch.Size([50, 1000])
06-Feb-25 20:55:48 - batch_labels.shape=torch.Size([50, 1000])
06-Feb-25 20:55:48 - Example input: One day, a little girl named Lily found a 
06-Feb-25 20:55:48 - Example labels: day, a little girl named Lily found a need
06-Feb-25 20:55:49 - [Epoch=1 | Batch=0] loss=8.0609
06-Feb-25 20:56:31 - [Epoch=1 | Batch=50] loss=4.4684
06-Feb-25 20:57:12 - [Epoch=1 | Batch=100] loss=4.3053
06-Feb-25 20:57:54 - [Epoch=1 | Batch=150] loss=4.2799
06-Feb-25 20:58:37 - [Epoch=1 | Batch=200] loss=4.1296
06-Feb-25 20:59:20 - [Epoch=1 | Batch=250] loss=3.9386
06-Feb-25 21:00:03 - [Epoch=1 | Batch=300] loss=4.0079
06-Feb-25 21:00:46 - [Epoch=1 | Batch=350] loss=3.6360
06-Feb-25 21:01:29 - [Epoch=1 | Batch=400] loss=3.8273
06-Feb-25 21:02:12 - [Epoch=1 | Batch=450] loss=3.8973
06-Feb-25 21:02:45 - ---------Epoch 01 | Loss: 3.6997
06-Feb-25 21:02:46 - [Epoch=2 | Batch=0] loss=3.8402
06-Feb-25 21:03:29 - [Epoch=2 | Batch=50] loss=3.5905
06-Feb-25 21:04:13 - [Epoch=2 | Batch=100] loss=3.7714
06-Feb-25 21:04:56 - [Epoch=2 | Batch=150] loss=3.8538
06-Feb-25 21:05:40 - [Epoch=2 | Batch=200] loss=3.8147
06-Feb-25 21:06:24 - [Epoch=2 | Batch=250] loss=3.6833
06-Feb-25 21:07:08 - [Epoch=2 | Batch=300] loss=3.7380
06-Feb-25 21:07:53 - [Epoch=2 | Batch=350] loss=3.4194
06-Feb-25 21:08:37 - [Epoch=2 | Batch=400] loss=3.5240
06-Feb-25 21:09:21 - [Epoch=2 | Batch=450] loss=3.6807
06-Feb-25 21:09:55 - ---------Epoch 02 | Loss: 3.5054
06-Feb-25 21:09:55 - [Epoch=3 | Batch=0] loss=3.6478
06-Feb-25 21:10:40 - [Epoch=3 | Batch=50] loss=3.4336
06-Feb-25 21:11:24 - [Epoch=3 | Batch=100] loss=3.6064
06-Feb-25 21:12:09 - [Epoch=3 | Batch=150] loss=3.6805
06-Feb-25 21:12:53 - [Epoch=3 | Batch=200] loss=3.6754
06-Feb-25 21:13:38 - [Epoch=3 | Batch=250] loss=3.4947
06-Feb-25 21:14:23 - [Epoch=3 | Batch=300] loss=3.5715
06-Feb-25 21:15:08 - [Epoch=3 | Batch=350] loss=3.2410
06-Feb-25 21:15:53 - [Epoch=3 | Batch=400] loss=3.3656
06-Feb-25 21:16:38 - [Epoch=3 | Batch=450] loss=3.5530
06-Feb-25 21:17:13 - ---------Epoch 03 | Loss: 3.3783
06-Feb-25 21:17:13 - [Epoch=4 | Batch=0] loss=3.5255
06-Feb-25 21:17:58 - [Epoch=4 | Batch=50] loss=3.2693
06-Feb-25 21:18:43 - [Epoch=4 | Batch=100] loss=3.4524
06-Feb-25 21:19:29 - [Epoch=4 | Batch=150] loss=3.5498
06-Feb-25 21:20:15 - [Epoch=4 | Batch=200] loss=3.5375
06-Feb-25 21:21:00 - [Epoch=4 | Batch=250] loss=3.3487
06-Feb-25 21:21:45 - [Epoch=4 | Batch=300] loss=3.4538
06-Feb-25 21:22:31 - [Epoch=4 | Batch=350] loss=3.1175
06-Feb-25 21:23:15 - [Epoch=4 | Batch=400] loss=3.2554
06-Feb-25 21:24:00 - [Epoch=4 | Batch=450] loss=3.4297
06-Feb-25 21:24:34 - ---------Epoch 04 | Loss: 3.2473
06-Feb-25 21:24:34 - [Epoch=5 | Batch=0] loss=3.3879
06-Feb-25 21:25:21 - [Epoch=5 | Batch=50] loss=3.1609
06-Feb-25 21:26:09 - [Epoch=5 | Batch=100] loss=3.3524
06-Feb-25 21:26:56 - [Epoch=5 | Batch=150] loss=3.4572
06-Feb-25 21:27:41 - [Epoch=5 | Batch=200] loss=3.4473
06-Feb-25 21:28:26 - [Epoch=5 | Batch=250] loss=3.2548
06-Feb-25 21:29:12 - [Epoch=5 | Batch=300] loss=3.3715
06-Feb-25 21:29:56 - [Epoch=5 | Batch=350] loss=3.0318
06-Feb-25 21:30:41 - [Epoch=5 | Batch=400] loss=3.1964
06-Feb-25 21:31:27 - [Epoch=5 | Batch=450] loss=3.3573
06-Feb-25 21:32:02 - ---------Epoch 05 | Loss: 3.1812
06-Feb-25 21:32:03 - [Epoch=6 | Batch=0] loss=3.3264
06-Feb-25 21:32:48 - [Epoch=6 | Batch=50] loss=3.0928
06-Feb-25 21:33:33 - [Epoch=6 | Batch=100] loss=3.2772
06-Feb-25 21:34:19 - [Epoch=6 | Batch=150] loss=3.3732
06-Feb-25 21:35:04 - [Epoch=6 | Batch=200] loss=3.3801
06-Feb-25 21:35:49 - [Epoch=6 | Batch=250] loss=3.2162
06-Feb-25 21:36:34 - [Epoch=6 | Batch=300] loss=3.3076
06-Feb-25 21:37:20 - [Epoch=6 | Batch=350] loss=2.9636
06-Feb-25 21:38:05 - [Epoch=6 | Batch=400] loss=3.0992
06-Feb-25 21:38:50 - [Epoch=6 | Batch=450] loss=3.2958
06-Feb-25 21:39:24 - ---------Epoch 06 | Loss: 3.1183
06-Feb-25 21:39:25 - [Epoch=7 | Batch=0] loss=3.2610
06-Feb-25 21:40:10 - [Epoch=7 | Batch=50] loss=3.0332
06-Feb-25 21:40:55 - [Epoch=7 | Batch=100] loss=3.2164
06-Feb-25 21:41:41 - [Epoch=7 | Batch=150] loss=3.3257
06-Feb-25 21:42:27 - [Epoch=7 | Batch=200] loss=3.3112
06-Feb-25 21:43:12 - [Epoch=7 | Batch=250] loss=3.1422
06-Feb-25 21:43:56 - [Epoch=7 | Batch=300] loss=3.2702
06-Feb-25 21:44:40 - [Epoch=7 | Batch=350] loss=2.9143
06-Feb-25 21:45:23 - [Epoch=7 | Batch=400] loss=3.0537
06-Feb-25 21:46:06 - [Epoch=7 | Batch=450] loss=3.2450
06-Feb-25 21:46:39 - ---------Epoch 07 | Loss: 3.0691
06-Feb-25 21:46:40 - [Epoch=8 | Batch=0] loss=3.2075
06-Feb-25 21:47:23 - [Epoch=8 | Batch=50] loss=2.9829
06-Feb-25 21:48:07 - [Epoch=8 | Batch=100] loss=3.1805
06-Feb-25 21:48:50 - [Epoch=8 | Batch=150] loss=3.2917
06-Feb-25 21:49:33 - [Epoch=8 | Batch=200] loss=3.2779
06-Feb-25 21:50:16 - [Epoch=8 | Batch=250] loss=3.0950
06-Feb-25 21:50:58 - [Epoch=8 | Batch=300] loss=3.2056
06-Feb-25 21:51:40 - [Epoch=8 | Batch=350] loss=2.8878
06-Feb-25 21:52:23 - [Epoch=8 | Batch=400] loss=3.0252
06-Feb-25 21:53:06 - [Epoch=8 | Batch=450] loss=3.2192
06-Feb-25 21:53:38 - ---------Epoch 08 | Loss: 3.0422
06-Feb-25 21:53:39 - [Epoch=9 | Batch=0] loss=3.1802
06-Feb-25 21:54:21 - [Epoch=9 | Batch=50] loss=2.9515
06-Feb-25 21:55:04 - [Epoch=9 | Batch=100] loss=3.1334
06-Feb-25 21:55:46 - [Epoch=9 | Batch=150] loss=3.2621
06-Feb-25 21:56:28 - [Epoch=9 | Batch=200] loss=3.2751
06-Feb-25 21:57:11 - [Epoch=9 | Batch=250] loss=3.0698
06-Feb-25 21:57:53 - [Epoch=9 | Batch=300] loss=3.1703
06-Feb-25 21:58:35 - [Epoch=9 | Batch=350] loss=2.8462
06-Feb-25 21:59:17 - [Epoch=9 | Batch=400] loss=2.9711
06-Feb-25 22:00:00 - [Epoch=9 | Batch=450] loss=3.1826
06-Feb-25 22:00:32 - ---------Epoch 09 | Loss: 3.0211
06-Feb-25 22:00:32 - [Epoch=10 | Batch=0] loss=3.1562
06-Feb-25 22:01:14 - [Epoch=10 | Batch=50] loss=2.9395
06-Feb-25 22:01:56 - [Epoch=10 | Batch=100] loss=3.1113
06-Feb-25 22:02:39 - [Epoch=10 | Batch=150] loss=3.2259
06-Feb-25 22:03:21 - [Epoch=10 | Batch=200] loss=3.2216
06-Feb-25 22:04:03 - [Epoch=10 | Batch=250] loss=3.0276
06-Feb-25 22:04:45 - [Epoch=10 | Batch=300] loss=3.1523
06-Feb-25 22:05:28 - [Epoch=10 | Batch=350] loss=2.8145
06-Feb-25 22:06:10 - [Epoch=10 | Batch=400] loss=2.9704
06-Feb-25 22:06:53 - [Epoch=10 | Batch=450] loss=3.1552
06-Feb-25 22:07:25 - ---------Epoch 10 | Loss: 2.9923
06-Feb-25 22:07:26 - Model weights saved
06-Feb-25 22:07:26 - Generated Text=Bloom lived in a big garden with lots of funny. He wanted to play with lots of funny. He asked his friends, but he could not see what he could not see what he could not see what he could not see what he could do not see what he could do not see what he could do not see what he could do not see what he could do not see what he could do not see what he could do not see what he could do not see what he could do not see what he could do not 
07-Feb-25 10:15:19 - Length of trainig data is  2119719
07-Feb-25 10:15:21 - Limiting training legth to 100000
07-Feb-25 10:15:21 - Training Non contextual tokeniser
07-Feb-25 10:15:26 - Sentence: The Cat sat on the Fence
07-Feb-25 10:15:26 - Tokens:  ['The▁', 'C', 'at▁', 'sat▁', 'on▁', 'the▁', 'F', 'en', 'ce▁']
07-Feb-25 10:15:26 - Token IDs: [60, 1947, 50, 1134, 56, 16, 1945, 23, 123]
07-Feb-25 10:15:26 - Token ids already present in file
07-Feb-25 10:15:30 - Total tokens:  24471344
07-Feb-25 10:15:30 - Resizing input_ids...
07-Feb-25 10:15:31 - input_ids.shape=torch.Size([1, 24471344])
07-Feb-25 10:15:31 - New shape:= torch.Size([24471, 1000])
07-Feb-25 10:15:31 - vocab_size=2000 embedding_dim/d_model=512
07-Feb-25 10:15:31 - Length of input ids =24471
07-Feb-25 10:15:33 - Training model...
07-Feb-25 10:15:33 - N= 24471 seq_length= 1000
07-Feb-25 10:15:33 - batch_input.shape=torch.Size([50, 1000])
07-Feb-25 10:15:33 - batch_labels.shape=torch.Size([50, 1000])
07-Feb-25 10:15:33 - Example input: One day, a little girl named Lily found a 
07-Feb-25 10:15:33 - Example labels: day, a little girl named Lily found a need
07-Feb-25 10:16:28 - Length of trainig data is  2119719
07-Feb-25 10:16:30 - Limiting training legth to 100000
07-Feb-25 10:16:30 - Training Non contextual tokeniser
07-Feb-25 10:16:35 - Sentence: The Cat sat on the Fence
07-Feb-25 10:16:35 - Tokens:  ['The▁', 'C', 'at▁', 'sat▁', 'on▁', 'the▁', 'F', 'en', 'ce▁']
07-Feb-25 10:16:35 - Token IDs: [60, 1947, 50, 1134, 56, 16, 1945, 23, 123]
07-Feb-25 10:16:35 - Token ids already present in file
07-Feb-25 10:16:38 - Total tokens:  24471344
07-Feb-25 10:16:38 - Resizing input_ids...
07-Feb-25 10:16:39 - input_ids.shape=torch.Size([1, 24471344])
07-Feb-25 10:16:39 - New shape:= torch.Size([24471, 1000])
07-Feb-25 10:16:39 - vocab_size=2000 embedding_dim/d_model=512
07-Feb-25 10:16:39 - Length of input ids =24471
07-Feb-25 10:16:40 - Training model...
07-Feb-25 10:16:40 - N= 24471 seq_length= 1000
07-Feb-25 10:16:40 - batch_input.shape=torch.Size([50, 1000])
07-Feb-25 10:16:40 - batch_labels.shape=torch.Size([50, 1000])
07-Feb-25 10:16:40 - Example input: One day, a little girl named Lily found a 
07-Feb-25 10:16:40 - Example labels: day, a little girl named Lily found a need
07-Feb-25 10:18:10 - Length of trainig data is  2119719
07-Feb-25 10:18:12 - Limiting training legth to 100000
07-Feb-25 10:18:12 - Training Non contextual tokeniser
07-Feb-25 10:18:17 - Sentence: The Cat sat on the Fence
07-Feb-25 10:18:17 - Tokens:  ['The▁', 'C', 'at▁', 'sat▁', 'on▁', 'the▁', 'F', 'en', 'ce▁']
07-Feb-25 10:18:17 - Token IDs: [60, 1947, 50, 1134, 56, 16, 1945, 23, 123]
07-Feb-25 10:18:17 - Token ids already present in file
07-Feb-25 10:18:21 - Total tokens:  24471344
07-Feb-25 10:18:21 - Resizing input_ids...
07-Feb-25 10:18:22 - input_ids.shape=torch.Size([1, 24471344])
07-Feb-25 10:18:22 - New shape:= torch.Size([24471, 1000])
07-Feb-25 10:18:22 - vocab_size=2000 embedding_dim/d_model=512
07-Feb-25 10:18:22 - Length of input ids =24471
07-Feb-25 10:18:23 - Training model...
07-Feb-25 10:18:23 - N= 24471 seq_length= 1000
07-Feb-25 10:18:23 - batch_input.shape=torch.Size([50, 1000])
07-Feb-25 10:18:23 - batch_labels.shape=torch.Size([50, 1000])
07-Feb-25 10:18:23 - Example input: One day, a little girl named Lily found a 
07-Feb-25 10:18:23 - Example labels: day, a little girl named Lily found a need
07-Feb-25 10:20:47 - Length of trainig data is  2119719
07-Feb-25 10:20:50 - Limiting training legth to 100000
07-Feb-25 10:20:50 - Training Non contextual tokeniser
07-Feb-25 10:20:55 - Sentence: The Cat sat on the Fence
07-Feb-25 10:20:55 - Tokens:  ['The▁', 'C', 'at▁', 'sat▁', 'on▁', 'the▁', 'F', 'en', 'ce▁']
07-Feb-25 10:20:55 - Token IDs: [60, 1947, 50, 1134, 56, 16, 1945, 23, 123]
07-Feb-25 10:20:55 - Token ids already present in file
07-Feb-25 10:20:58 - Total tokens:  24471344
07-Feb-25 10:20:58 - Resizing input_ids...
07-Feb-25 10:20:59 - input_ids.shape=torch.Size([1, 24471344])
07-Feb-25 10:20:59 - New shape:= torch.Size([24471, 1000])
07-Feb-25 10:20:59 - vocab_size=2000 embedding_dim/d_model=512
07-Feb-25 10:20:59 - Length of input ids =24471
07-Feb-25 10:21:01 - Training model...
07-Feb-25 10:21:01 - N= 24471 seq_length= 1000
07-Feb-25 10:21:01 - batch_input.shape=torch.Size([50, 1000])
07-Feb-25 10:21:01 - batch_labels.shape=torch.Size([50, 1000])
07-Feb-25 10:21:01 - Example input: One day, a little girl named Lily found a 
07-Feb-25 10:21:01 - Example labels: day, a little girl named Lily found a need
07-Feb-25 10:21:02 - [Epoch=1 | Batch=0] loss=8.1205
07-Feb-25 10:21:30 - Length of trainig data is  2119719
07-Feb-25 10:21:32 - Limiting training legth to 100000
07-Feb-25 10:21:32 - Training Non contextual tokeniser
07-Feb-25 10:21:37 - Sentence: The Cat sat on the Fence
07-Feb-25 10:21:37 - Tokens:  ['The▁', 'C', 'at▁', 'sat▁', 'on▁', 'the▁', 'F', 'en', 'ce▁']
07-Feb-25 10:21:37 - Token IDs: [60, 1947, 50, 1134, 56, 16, 1945, 23, 123]
07-Feb-25 10:21:37 - Token ids already present in file
07-Feb-25 10:21:40 - Total tokens:  24471344
07-Feb-25 10:21:40 - Resizing input_ids...
07-Feb-25 10:21:41 - input_ids.shape=torch.Size([1, 24471344])
07-Feb-25 10:21:41 - New shape:= torch.Size([24471, 1000])
07-Feb-25 10:21:41 - vocab_size=2000 embedding_dim/d_model=512
07-Feb-25 10:21:41 - Length of input ids =24471
07-Feb-25 10:21:42 - Training model...
07-Feb-25 10:21:42 - N= 24471 seq_length= 1000
07-Feb-25 10:21:42 - batch_input.shape=torch.Size([25, 1000])
07-Feb-25 10:21:42 - batch_labels.shape=torch.Size([25, 1000])
07-Feb-25 10:21:42 - Example input: One day, a little girl named Lily found a 
07-Feb-25 10:21:42 - Example labels: day, a little girl named Lily found a need
07-Feb-25 10:21:43 - [Epoch=1 | Batch=0] loss=8.0742
07-Feb-25 10:22:06 - [Epoch=1 | Batch=50] loss=4.5992
07-Feb-25 10:22:28 - [Epoch=1 | Batch=100] loss=3.9967
07-Feb-25 10:22:51 - [Epoch=1 | Batch=150] loss=3.8894
07-Feb-25 10:23:14 - [Epoch=1 | Batch=200] loss=4.2689
07-Feb-25 10:23:37 - [Epoch=1 | Batch=250] loss=3.9901
07-Feb-25 10:24:01 - [Epoch=1 | Batch=300] loss=4.1495
07-Feb-25 10:24:24 - [Epoch=1 | Batch=350] loss=4.0864
07-Feb-25 10:24:48 - [Epoch=1 | Batch=400] loss=4.0537
07-Feb-25 10:25:11 - [Epoch=1 | Batch=450] loss=3.9795
07-Feb-25 10:25:35 - [Epoch=1 | Batch=500] loss=3.8604
07-Feb-25 10:25:58 - [Epoch=1 | Batch=550] loss=3.8889
07-Feb-25 10:26:22 - [Epoch=1 | Batch=600] loss=3.9448
07-Feb-25 10:26:46 - [Epoch=1 | Batch=650] loss=3.8633
07-Feb-25 10:27:10 - [Epoch=1 | Batch=700] loss=3.6324
07-Feb-25 10:27:34 - [Epoch=1 | Batch=750] loss=3.8459
07-Feb-25 10:27:58 - [Epoch=1 | Batch=800] loss=3.6554
07-Feb-25 10:28:22 - [Epoch=1 | Batch=850] loss=3.4670
07-Feb-25 10:28:46 - [Epoch=1 | Batch=900] loss=3.7350
07-Feb-25 10:29:11 - [Epoch=1 | Batch=950] loss=3.8335
07-Feb-25 10:29:24 - ---------Epoch 01 | Loss: 3.3663
07-Feb-25 10:29:25 - [Epoch=2 | Batch=0] loss=3.6414
07-Feb-25 10:29:49 - [Epoch=2 | Batch=50] loss=3.6296
07-Feb-25 10:30:13 - [Epoch=2 | Batch=100] loss=3.2735
07-Feb-25 10:30:38 - [Epoch=2 | Batch=150] loss=3.2251
07-Feb-25 10:31:03 - [Epoch=2 | Batch=200] loss=3.7301
07-Feb-25 10:31:28 - [Epoch=2 | Batch=250] loss=3.5618
07-Feb-25 10:31:53 - [Epoch=2 | Batch=300] loss=3.6978
07-Feb-25 10:32:18 - [Epoch=2 | Batch=350] loss=3.6695
07-Feb-25 10:32:45 - [Epoch=2 | Batch=400] loss=3.6717
07-Feb-25 10:33:10 - [Epoch=2 | Batch=450] loss=3.5942
07-Feb-25 10:33:36 - [Epoch=2 | Batch=500] loss=3.4628
07-Feb-25 10:34:01 - [Epoch=2 | Batch=550] loss=3.5135
07-Feb-25 10:34:26 - [Epoch=2 | Batch=600] loss=3.5668
07-Feb-25 10:34:52 - [Epoch=2 | Batch=650] loss=3.4832
07-Feb-25 10:35:17 - [Epoch=2 | Batch=700] loss=3.2857
07-Feb-25 10:35:42 - [Epoch=2 | Batch=750] loss=3.5055
07-Feb-25 10:36:08 - [Epoch=2 | Batch=800] loss=3.2523
07-Feb-25 10:36:34 - [Epoch=2 | Batch=850] loss=3.1115
07-Feb-25 10:36:59 - [Epoch=2 | Batch=900] loss=3.3782
07-Feb-25 10:37:23 - [Epoch=2 | Batch=950] loss=3.4749
07-Feb-25 10:37:37 - ---------Epoch 02 | Loss: 3.0139
07-Feb-25 10:37:37 - [Epoch=3 | Batch=0] loss=3.2876
07-Feb-25 10:38:03 - [Epoch=3 | Batch=50] loss=3.3287
07-Feb-25 10:38:28 - [Epoch=3 | Batch=100] loss=2.9215
07-Feb-25 10:38:54 - [Epoch=3 | Batch=150] loss=2.9336
07-Feb-25 10:39:21 - [Epoch=3 | Batch=200] loss=3.4460
07-Feb-25 10:39:48 - [Epoch=3 | Batch=250] loss=3.2944
07-Feb-25 10:40:14 - [Epoch=3 | Batch=300] loss=3.3940
07-Feb-25 10:40:41 - [Epoch=3 | Batch=350] loss=3.4068
07-Feb-25 10:41:08 - [Epoch=3 | Batch=400] loss=3.4231
07-Feb-25 10:41:34 - [Epoch=3 | Batch=450] loss=3.3597
07-Feb-25 10:42:01 - [Epoch=3 | Batch=500] loss=3.2345
07-Feb-25 10:42:28 - [Epoch=3 | Batch=550] loss=3.3162
07-Feb-25 10:42:54 - [Epoch=3 | Batch=600] loss=3.3445
07-Feb-25 10:43:21 - [Epoch=3 | Batch=650] loss=3.2664
07-Feb-25 10:43:47 - [Epoch=3 | Batch=700] loss=3.0612
07-Feb-25 10:44:14 - [Epoch=3 | Batch=750] loss=3.3250
07-Feb-25 10:44:41 - [Epoch=3 | Batch=800] loss=3.0382
07-Feb-25 10:45:09 - [Epoch=3 | Batch=850] loss=2.8967
07-Feb-25 10:45:37 - [Epoch=3 | Batch=900] loss=3.1981
07-Feb-25 10:46:04 - [Epoch=3 | Batch=950] loss=3.2983
07-Feb-25 10:46:19 - ---------Epoch 03 | Loss: 2.8567
07-Feb-25 10:46:19 - [Epoch=4 | Batch=0] loss=3.1279
07-Feb-25 10:46:47 - [Epoch=4 | Batch=50] loss=3.1618
07-Feb-25 10:47:14 - [Epoch=4 | Batch=100] loss=2.7902
07-Feb-25 10:47:42 - [Epoch=4 | Batch=150] loss=2.7951
07-Feb-25 10:48:09 - [Epoch=4 | Batch=200] loss=3.3046
07-Feb-25 10:48:34 - [Epoch=4 | Batch=250] loss=3.1495
07-Feb-25 10:49:00 - [Epoch=4 | Batch=300] loss=3.2378
07-Feb-25 10:49:25 - [Epoch=4 | Batch=350] loss=3.2684
07-Feb-25 10:49:50 - [Epoch=4 | Batch=400] loss=3.2926
07-Feb-25 10:50:16 - [Epoch=4 | Batch=450] loss=3.2214
07-Feb-25 10:50:41 - [Epoch=4 | Batch=500] loss=3.1014
07-Feb-25 10:51:06 - [Epoch=4 | Batch=550] loss=3.1698
07-Feb-25 10:51:31 - [Epoch=4 | Batch=600] loss=3.2194
07-Feb-25 10:51:57 - [Epoch=4 | Batch=650] loss=3.1398
07-Feb-25 10:52:21 - [Epoch=4 | Batch=700] loss=2.9544
07-Feb-25 10:52:46 - [Epoch=4 | Batch=750] loss=3.2252
07-Feb-25 10:53:11 - [Epoch=4 | Batch=800] loss=2.9205
07-Feb-25 10:53:35 - [Epoch=4 | Batch=850] loss=2.8059
07-Feb-25 10:53:59 - [Epoch=4 | Batch=900] loss=3.1009
07-Feb-25 10:54:23 - [Epoch=4 | Batch=950] loss=3.1848
07-Feb-25 10:54:36 - ---------Epoch 04 | Loss: 2.7601
07-Feb-25 10:54:36 - [Epoch=5 | Batch=0] loss=3.0274
07-Feb-25 10:55:00 - [Epoch=5 | Batch=50] loss=3.0812
07-Feb-25 10:55:24 - [Epoch=5 | Batch=100] loss=2.6951
07-Feb-25 10:55:48 - [Epoch=5 | Batch=150] loss=2.7072
07-Feb-25 10:56:12 - [Epoch=5 | Batch=200] loss=3.1984
07-Feb-25 10:56:35 - [Epoch=5 | Batch=250] loss=3.0481
07-Feb-25 10:56:59 - [Epoch=5 | Batch=300] loss=3.1559
07-Feb-25 10:57:23 - [Epoch=5 | Batch=350] loss=3.1799
07-Feb-25 10:57:47 - [Epoch=5 | Batch=400] loss=3.1980
07-Feb-25 10:58:10 - [Epoch=5 | Batch=450] loss=3.1470
07-Feb-25 10:58:34 - [Epoch=5 | Batch=500] loss=3.0190
07-Feb-25 10:58:57 - [Epoch=5 | Batch=550] loss=3.0691
07-Feb-25 10:59:21 - [Epoch=5 | Batch=600] loss=3.1434
07-Feb-25 10:59:45 - [Epoch=5 | Batch=650] loss=3.0738
07-Feb-25 11:00:09 - [Epoch=5 | Batch=700] loss=2.8712
07-Feb-25 11:00:32 - [Epoch=5 | Batch=750] loss=3.1179
07-Feb-25 11:00:56 - [Epoch=5 | Batch=800] loss=2.8127
07-Feb-25 11:01:19 - [Epoch=5 | Batch=850] loss=2.6983
07-Feb-25 11:01:43 - [Epoch=5 | Batch=900] loss=2.9958
07-Feb-25 11:02:07 - [Epoch=5 | Batch=950] loss=3.1055
07-Feb-25 11:02:19 - ---------Epoch 05 | Loss: 2.6791
07-Feb-25 11:02:20 - [Epoch=6 | Batch=0] loss=2.9228
07-Feb-25 11:02:43 - [Epoch=6 | Batch=50] loss=2.9993
07-Feb-25 11:03:07 - [Epoch=6 | Batch=100] loss=2.6337
07-Feb-25 11:03:31 - [Epoch=6 | Batch=150] loss=2.6586
07-Feb-25 11:03:54 - [Epoch=6 | Batch=200] loss=3.1397
07-Feb-25 11:04:18 - [Epoch=6 | Batch=250] loss=3.0045
07-Feb-25 11:04:41 - [Epoch=6 | Batch=300] loss=3.0827
07-Feb-25 11:05:05 - [Epoch=6 | Batch=350] loss=3.1158
07-Feb-25 11:05:29 - [Epoch=6 | Batch=400] loss=3.1248
07-Feb-25 11:05:52 - [Epoch=6 | Batch=450] loss=3.0787
07-Feb-25 11:06:16 - [Epoch=6 | Batch=500] loss=2.9755
07-Feb-25 11:06:39 - [Epoch=6 | Batch=550] loss=3.0201
07-Feb-25 11:07:03 - [Epoch=6 | Batch=600] loss=3.0962
07-Feb-25 11:07:27 - [Epoch=6 | Batch=650] loss=3.0231
07-Feb-25 11:07:50 - [Epoch=6 | Batch=700] loss=2.8212
07-Feb-25 11:08:14 - [Epoch=6 | Batch=750] loss=3.0517
07-Feb-25 11:08:37 - [Epoch=6 | Batch=800] loss=2.7631
07-Feb-25 11:09:01 - [Epoch=6 | Batch=850] loss=2.6791
07-Feb-25 11:09:24 - [Epoch=6 | Batch=900] loss=2.9710
07-Feb-25 11:09:48 - [Epoch=6 | Batch=950] loss=3.0417
07-Feb-25 11:10:01 - ---------Epoch 06 | Loss: 2.6135
07-Feb-25 11:10:01 - [Epoch=7 | Batch=0] loss=2.8631
07-Feb-25 11:10:25 - [Epoch=7 | Batch=50] loss=2.9283
07-Feb-25 11:10:48 - [Epoch=7 | Batch=100] loss=2.5605
07-Feb-25 11:11:12 - [Epoch=7 | Batch=150] loss=2.5672
07-Feb-25 11:11:35 - [Epoch=7 | Batch=200] loss=3.0805
07-Feb-25 11:11:59 - [Epoch=7 | Batch=250] loss=2.9546
07-Feb-25 11:12:22 - [Epoch=7 | Batch=300] loss=3.0347
07-Feb-25 11:12:46 - [Epoch=7 | Batch=350] loss=3.0700
07-Feb-25 11:13:09 - [Epoch=7 | Batch=400] loss=3.0700
07-Feb-25 11:13:33 - [Epoch=7 | Batch=450] loss=3.0252
07-Feb-25 11:13:56 - [Epoch=7 | Batch=500] loss=2.9285
07-Feb-25 11:14:20 - [Epoch=7 | Batch=550] loss=2.9712
07-Feb-25 11:14:44 - [Epoch=7 | Batch=600] loss=3.0300
07-Feb-25 11:15:07 - [Epoch=7 | Batch=650] loss=2.9563
07-Feb-25 11:15:31 - [Epoch=7 | Batch=700] loss=2.7645
07-Feb-25 11:15:54 - [Epoch=7 | Batch=750] loss=3.0029
07-Feb-25 11:16:18 - [Epoch=7 | Batch=800] loss=2.6890
07-Feb-25 11:16:41 - [Epoch=7 | Batch=850] loss=2.6107
07-Feb-25 11:17:05 - [Epoch=7 | Batch=900] loss=2.9271
07-Feb-25 11:17:28 - [Epoch=7 | Batch=950] loss=2.9955
07-Feb-25 11:17:41 - ---------Epoch 07 | Loss: 2.5770
07-Feb-25 11:17:42 - [Epoch=8 | Batch=0] loss=2.8311
07-Feb-25 11:18:05 - [Epoch=8 | Batch=50] loss=2.9046
07-Feb-25 11:18:29 - [Epoch=8 | Batch=100] loss=2.5404
07-Feb-25 11:18:52 - [Epoch=8 | Batch=150] loss=2.5412
07-Feb-25 11:19:16 - [Epoch=8 | Batch=200] loss=3.0538
07-Feb-25 11:19:40 - [Epoch=8 | Batch=250] loss=2.9078
07-Feb-25 11:20:03 - [Epoch=8 | Batch=300] loss=2.9810
07-Feb-25 11:20:27 - [Epoch=8 | Batch=350] loss=3.0231
07-Feb-25 11:20:50 - [Epoch=8 | Batch=400] loss=3.0457
07-Feb-25 11:21:14 - [Epoch=8 | Batch=450] loss=2.9856
07-Feb-25 11:21:37 - [Epoch=8 | Batch=500] loss=2.8666
07-Feb-25 11:22:01 - [Epoch=8 | Batch=550] loss=2.9288
07-Feb-25 11:22:25 - [Epoch=8 | Batch=600] loss=2.9813
07-Feb-25 11:22:48 - [Epoch=8 | Batch=650] loss=2.9223
07-Feb-25 11:23:12 - [Epoch=8 | Batch=700] loss=2.7378
07-Feb-25 11:23:35 - [Epoch=8 | Batch=750] loss=2.9776
07-Feb-25 11:23:58 - [Epoch=8 | Batch=800] loss=2.6550
07-Feb-25 11:24:22 - [Epoch=8 | Batch=850] loss=2.5716
07-Feb-25 11:24:45 - [Epoch=8 | Batch=900] loss=2.8703
07-Feb-25 11:25:09 - [Epoch=8 | Batch=950] loss=2.9477
07-Feb-25 11:25:22 - ---------Epoch 08 | Loss: 2.5354
07-Feb-25 11:25:22 - [Epoch=9 | Batch=0] loss=2.7881
07-Feb-25 11:25:46 - [Epoch=9 | Batch=50] loss=2.8545
07-Feb-25 11:26:09 - [Epoch=9 | Batch=100] loss=2.4630
07-Feb-25 11:26:33 - [Epoch=9 | Batch=150] loss=2.4902
07-Feb-25 11:26:57 - [Epoch=9 | Batch=200] loss=3.0097
07-Feb-25 11:27:20 - [Epoch=9 | Batch=250] loss=2.8684
07-Feb-25 11:27:44 - [Epoch=9 | Batch=300] loss=2.9476
07-Feb-25 11:28:07 - [Epoch=9 | Batch=350] loss=2.9932
07-Feb-25 11:28:31 - [Epoch=9 | Batch=400] loss=2.9946
07-Feb-25 11:28:54 - [Epoch=9 | Batch=450] loss=2.9604
07-Feb-25 11:29:18 - [Epoch=9 | Batch=500] loss=2.8491
07-Feb-25 11:29:41 - [Epoch=9 | Batch=550] loss=2.9019
07-Feb-25 11:30:05 - [Epoch=9 | Batch=600] loss=2.9482
07-Feb-25 11:30:29 - [Epoch=9 | Batch=650] loss=2.8908
07-Feb-25 11:30:52 - [Epoch=9 | Batch=700] loss=2.7142
07-Feb-25 11:31:16 - [Epoch=9 | Batch=750] loss=2.9429
07-Feb-25 11:31:39 - [Epoch=9 | Batch=800] loss=2.6253
07-Feb-25 11:32:03 - [Epoch=9 | Batch=850] loss=2.5457
07-Feb-25 11:32:26 - [Epoch=9 | Batch=900] loss=2.8537
07-Feb-25 11:32:50 - [Epoch=9 | Batch=950] loss=2.9179
07-Feb-25 11:33:03 - ---------Epoch 09 | Loss: 2.4942
07-Feb-25 11:33:03 - [Epoch=10 | Batch=0] loss=2.7491
07-Feb-25 11:33:27 - [Epoch=10 | Batch=50] loss=2.8464
07-Feb-25 11:33:50 - [Epoch=10 | Batch=100] loss=2.4415
07-Feb-25 11:34:14 - [Epoch=10 | Batch=150] loss=2.4769
07-Feb-25 11:34:37 - [Epoch=10 | Batch=200] loss=2.9730
07-Feb-25 11:35:01 - [Epoch=10 | Batch=250] loss=2.8467
07-Feb-25 11:35:24 - [Epoch=10 | Batch=300] loss=2.9087
07-Feb-25 11:35:48 - [Epoch=10 | Batch=350] loss=2.9588
07-Feb-25 11:36:11 - [Epoch=10 | Batch=400] loss=2.9807
07-Feb-25 11:36:34 - [Epoch=10 | Batch=450] loss=2.9274
07-Feb-25 11:36:58 - [Epoch=10 | Batch=500] loss=2.8267
07-Feb-25 11:37:21 - [Epoch=10 | Batch=550] loss=2.8716
07-Feb-25 11:37:45 - [Epoch=10 | Batch=600] loss=2.9358
07-Feb-25 11:38:08 - [Epoch=10 | Batch=650] loss=2.8652
07-Feb-25 11:38:32 - [Epoch=10 | Batch=700] loss=2.6854
07-Feb-25 11:38:55 - [Epoch=10 | Batch=750] loss=2.9105
07-Feb-25 11:39:19 - [Epoch=10 | Batch=800] loss=2.5720
07-Feb-25 11:39:42 - [Epoch=10 | Batch=850] loss=2.5104
07-Feb-25 11:40:06 - [Epoch=10 | Batch=900] loss=2.8228
07-Feb-25 11:40:29 - [Epoch=10 | Batch=950] loss=2.8939
07-Feb-25 11:40:42 - ---------Epoch 10 | Loss: 2.4679
07-Feb-25 11:40:42 - Model weights saved
